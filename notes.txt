npm init -> initialising
we can change script section in package.json
for ex:-
"scripts": {
    "start":"node index.js"
  }
now lets install express:- npm i express

lets install dotenv:- npm i dotenv
now make .env file, here we right ports, api keys, urls to secure your application by not pushing .env file into github

let save port number in .env
PORT=3000

Now to use this:-
go to the file where you want to use this port

add require :- require('dotenv').config();
and to use:- process.env.name

for example replace
app.listen(port,()=>{
    console.log(`app listening on port ${port}`);
})
to 
app.listen(process.env.PORT,()=>{
    console.log(`app listening on port ${port}`);
}) 

-> we are using import express statement instead of require. Import comes under module express.
so we need to add "type":'module' in package.json
other wise it gives error.

lets make a joke array which act as a simple api, then we fill get it in the frontend part

//* axios
 ### ðŸ“¦ What is **Axios**?

**Axios** is a **promise-based HTTP client** for making requests to servers (APIs) from the browser or Node.js.

---

### âœ… **Key Features of Axios**

| Feature                          | Description                                              |
| -------------------------------- | -------------------------------------------------------- |
| ðŸ”„ Promise-based                 | Works seamlessly with `async/await` or `.then().catch()` |
| ðŸŒ HTTP Requests                 | Supports GET, POST, PUT, DELETE, PATCH, etc.             |
| ðŸ“¥ Automatic JSON Handling       | Converts response data into JSON automatically           |
| ðŸ§¾ Request/Response Interceptors | You can modify requests or responses globally            |
| ðŸ§  Error Handling                | Axios gives detailed errors if something fails           |
| ðŸ” Supports Headers & Auth       | Allows custom headers, tokens, etc.                      |
| ðŸŒ Supports CORS                 | Can work with cross-origin APIs (if backend allows it)   |

---

ðŸ§ª Example Usage

ðŸ”¹ GET Request

```js
import axios from 'axios';

axios.get('https://api.example.com/data')
  .then(response => {
    console.log(response.data);  // the response payload
  })
  .catch(error => {
    console.error(error);
  });
```
ðŸ”¹ POST Request

```js
axios.post('https://api.example.com/login', {
  username: 'john',
  password: 'secret'
})
.then(response => {
  console.log(response.data);
})
.catch(error => {
  console.error(error);
});
```

### ðŸ“¥ How to Install Axios

```bash
npm install axios
```


---

### ðŸ” Axios vs Fetch

| Feature         | Axios                 | Fetch API                         |
| --------------- | --------------------- | --------------------------------- |
| Default JSON    | âœ… Auto JSON parse     | âŒ You must manually use `.json()` |
| Error Handling  | âœ… Catches HTTP errors | âŒ Only network errors caught      |
| Interceptors    | âœ… Yes                 | âŒ No                              |
| Browser Support | âœ… Good                | âœ… Built-in in modern browsers     |



 //* ðŸŒWhat is **CORS**?

CORS stands for Cross-Origin Resource Sharing.
-> ghr me sbko allow mtt kro, sirf jinko jante ho unhe hi kro

Itâ€™s a **security feature** implemented by browsers to **control how web pages access resources from different origins**.

---
ðŸ§  **Understanding the Problem**

A **webpage** is said to be from an **origin** defined by:

**Protocol** (http/https)
**Domain** (example.com)
**Port** (3000, 4000, etc.)

So:

```plaintext
http://localhost:3000   // frontend
http://localhost:4000   // backend
```

These are considered **different origins**.

ðŸš« **Why CORS is Needed**

Browsers **block requests** from one origin to another **by default** (for security).
For example:

```js
// This will be blocked unless backend allows it
axios.get("http://localhost:4000/data") 
```

The backend must **explicitly allow** requests from the frontend's origin.

---
âœ… **How to Fix CORS**

In a Node.js/Express backend, use the `cors` middleware:

```bash
npm install cors
```

```js
const express = require('express');
const cors = require('cors');
const app = express();

app.use(cors());  // allows all origins by default

// OR for specific origin
app.use(cors({ origin: 'http://localhost:3000' }));

app.get('/data', (req, res) => {
  res.json({ message: 'Hello from backend' });
});
```

ðŸ” Why CORS Exists

To **prevent malicious scripts** from sending requests to another site (like your bank) and accessing private data.

ðŸ§ª Simple Example

Without CORS:

Frontend â†’ Request to `http://localhost:4000/jokes` â†’ âŒ **Blocked by browser**

With CORS enabled on backend:

Frontend â†’ Request to `http://localhost:4000/jokes` â†’ âœ… **Allowed**

ðŸ“Œ Summary

| Term     | Meaning                                                        |
| -------- | -------------------------------------------------------------- |
| **CORS** | Cross-Origin Resource Sharing                                  |
| **Why**  | Prevent unauthorized cross-site access                         |
| **Fix**  | Backend must send proper `Access-Control-Allow-Origin` headers |

//* if you dont want to add CORS at local level, u can use proxy as well

//* âš™ï¸ What is **Proxy** in Vite (React)?

In **Vite + React** development, a **proxy** is used to **forward API requests** from the frontend (React app) to the backend (Node/Express, etc.) â€” especially when they're running on **different ports** (causing **CORS issues**).

ðŸ”¥ Why Use Proxy?

Imagine:

* Frontend: `http://localhost:5173`
* Backend: `http://localhost:4000`

If your React app makes an API call like:

axios.get("http://localhost:4000/jokes")

It will cause a **CORS error** in the browser unless the backend explicitly allows it.

> âœ… Instead of setting CORS on the backend during development, Vite lets you **proxy** requests through the frontend.

ðŸ“ Vite Proxy Setup (in `vite.config.js`)

// vite.config.js
export default {
  server: {
    proxy: {
      '/api': {
        target: 'http://localhost:4000',
        changeOrigin: true,
        rewrite: (path) => path.replace(/^\/api/, ''),
      },
    },
  },
};


ðŸ“¥ Now Use in React:


axios.get('/api/jokes')  // Internally proxies to http://localhost:4000/jokes

You no longer need to call the full backend URL, and **no CORS error will happen**.

âœ… Benefits of Proxy in Vite

| Benefit                   | Description                                          |
| ------------------------- | ---------------------------------------------------- |
| ðŸ›¡ï¸ Avoid CORS during dev | No need to modify backend CORS settings              |
| ðŸ§¹ Cleaner API calls      | Use relative paths like `/api/jokes`                 |
| âš¡ Faster development      | Backend and frontend stay separate but work together |

---

### â—Note

* This only works in **development mode** (`npm run dev`).
* For **production**, you must configure proper **CORS headers** on the backend, or use a **reverse proxy** (like Nginx).

//* Day 3rd
we can use Moon Modeler:- is a data modeling tool for MongoDB and noSQL or can use eraser.io
for drawing the structure of the projet

//* about Modelling
we use mongoose for Modelling
npm i mongoose

//making a model
we need three steps to make any model
1. import mongoose
    *** import mongoose from "mongoose" ***

2. create a schema
   *** const userScehma= new mongoose.Schema({}) ***
    
3. create a model
   *** export const User=mongoose.model("User", userSchema); ***

we make model using mongoose i.e mongoose.model(); and when this model get connected to database , these file automatically run first and make a structure in mongoDB

* impt point- 
as we have declare model name as User here but in monodb it becomes plural and in small letters that is, users.

*** Data Modelling :-
Data modelling is the process of creating a conceptual representation of data, including its structure, relationships, and constraints. It involves defining the entities, attributes, and relationships within a data model, which serves as a blueprint for designing and implementing databases, data warehouses, and other data storage systems.

we model data in schema
const userSchema = new mongoose.Schema({
  email: String,

  -> we can also define object which calls mongoose superPower, this also act as a validation part

  username: {
    type: String,
    required: true,
    //if want name to be unique
    unique: true,
    // for lowercase
    lowercase: true,
  }

  ->we can also create arrays
  subTodos:[
    {
      // we can also use another model reference
      type: mongoose.Schema.Types.ObjectId,
      ref: 'Todo',
    },
  ]

  password: String,
  });
  // we can add more fields here

//* to use createdAt, updateAt and all time related function ,mongoose provide as timestamp keyword

const userSchema = new mongoose.Schema({},{timestamps:true});

** starting backend Professionally

impt point-> if we ever mae folder inside folder , git never track it, as it only tracks file. For that we make .gitKeep file, then git track the folders aswell and we can simply push them.

lets make
 
*** .gitignore  file:- 
used to add those file which should not be pushed into github.
You can generate the data for gitignore from any gitignore generator

*** .env file ***
used to store the environment variables. we can store the database url, api keys etc.

*** package.json file ***
used to store the project dependencies and scripts. we can install the dependencies using npm install

//* Nodemon: - 
instead manually starting your server, nodemon refresh the server every time you write code
npm i -D nodemon

"scripts": {
    "dev": "nodemon src/index.js"
  },

adding script that use nodemon for index.js file 

//* we are not going to use require method instead we are going to use import method

so change the type in package.json
"type": "module"

//* we'll create some folders inside src folder
controllers
1. db - for writing logic for connecting databse
2. middlewares - for codes that are required to run in between. Like if any request came and before request fullsiled by server if we want some checkings , then we use middleware. 
3. models - for database schema
4. routes - for defining routes
5. services - for business logic
6. utils - for utility functions that are required multiple times like mailing, token exchanges.
7. validators - for validation of data
8. helpers - for helper functions that are required multiple times like logging, error handling.

//* Prettier
prettier is a code formatter. It formats your code to follow the style guide. It is useful for maintaining a consistent code style throughout the project.
npm i -D prettier

we need to add some file manually for pretttier
1. .prettierrc
    {
    "singleQuote": false,
    "doubleQuote": true,
    "bracketSpacing": true,
    "trailingComma": "es5",
    "tabWidth": 2,
    "semi": true
}

2. .prettierignore - in which files i dont want to implement prettier

** Starting with MongoDB
1. adding in to .env file
MONGO_URI=mongodb://localhost:27017/
2. create a db name and add in constants.js because if in future we need to change its name ,we donot require to go to all files to change the name, simply change here only.

we can add db in two ways, either write directly in index.js as index.js is the one who runs first or make separate db folder and add file and import into index.js for clean coding.

install mongoose, express and dotenv
npm i mongoose express dotenv

** impt point
1. whenever we try to talk to data base, many error comes so we should use either try and catch error handling or promises.
2. we should use async await method for promises. Because it makes code clean and easy to read.
3. we should use middleware for error handling. Because it makes code clean and easy to read.

3. importing mongoose
   import mongoose from "mongoose";

we usually create db and coonect it like

 function connectDB(){}

 connectDB()

but instead this we can use ifi method
    ()()
  this means this is our function and will immediatly runs

  we use async, so

import mongoose from "mongoose";
import {DB_NAME} from "./constants";
import express from "express";

const app=express()

//making connection
  ( async()=>{
   
    //use try catch every time u use db
    try {
      **connection db
      await mongoose.connect(`${process.env.MONGO_URI}/${DB_NAME}`);
      //using some feature of express
      app.on("error",(error)=>{
        console.log(error);
        throw error
      })
      //we can use listen as well
      app.listen(proces.env.PORT,()=>{
        console.log("server is running on port 3000");
      })

    },
    catch(error){
      console.error("Eoor", error)
      throw error
    }
  })()

  some times people use ; before ifi as if the previous line doesnot have ; some how, then it will create error

  ;()()

** 2nd approach, making separate db file then importing in index.js

import mongoose from "mongoose";
import {DB_NAME} from "./constants";

const connectDB= async()=>{
  try {
    //we can store the connection as well in some variables
   const ConnectionInstance = await mongoose.connect(`${process.env.MONGO_URI}/${DB_NAME}`);
   console.log(`\n mongoDB connected !! DB HOST: ${ConnectionInstance.connection.host}`);
  }
  catch(error)
  {
    console.error("Error", error);
    //can also use exit method given offered process
    process.exit(1);
  }
}

export default connectDB;

//importing in index.js
import connectDB from "./db.js";

connectDB();

//**
we should import dotenv file as well in index.js as it load all environment variable as fast as possible

import dotenv from "dotenv";
dotenv.config({
  path: "./config.env"
});

this is a new method so we need to do some changes in package.json
"scripts": {
    "dev": "nodemon -r dotenv/config --experimental src/index.js"
  },

*** impt-> we use app.use() when we need to use middleware or configuration
*** impt-> we use app.get() when we need to use route
*** impt-> we use app.post() when we need to use route
*** impt-> we use app.put() when we need to use route
*** impt-> we use app.delete() when we need to use route
*** impt-> we use app.all() when we need to use route

//* cookies parser
`cookie-parser` is a middleware in Node.js (especially used with Express.js) that **parses cookies** attached to the client request object and makes them easily accessible via `req.cookies`.

### ðŸ”§ Why Use `cookie-parser`?

When a browser sends a request to a server, it includes cookies in the HTTP headers. However, in raw form, cookies are just a string, like this:

```
Cookie: token=abc123; theme=dark
```

Without `cookie-parser`, you'd need to manually parse this string to get individual cookie values. `cookie-parser` simplifies that process.

---

### âœ… How to Install

```bash
npm install cookie-parser
```

---

### ðŸ“¦ Basic Usage in Express

```js
const express = require('express');
const cookieParser = require('cookie-parser');

const app = express();

// Use the middleware
app.use(cookieParser());

// Access cookies
app.get('/', (req, res) => {
  console.log(req.cookies); // { token: 'abc123', theme: 'dark' }
  res.send('Cookies parsed!');
});
```

---

### ðŸ” Using with a Secret (for signed cookies)

```js
app.use(cookieParser('mySecretKey'));

// Setting a signed cookie
res.cookie('token', 'abc123', { signed: true });

// Accessing signed cookies
console.log(req.signedCookies.token); // 'abc123'
```

---

### ðŸ” Summary

| Feature        | What it does                                              |
| -------------- | --------------------------------------------------------- |
| Parses cookies | Parses `Cookie` header into `req.cookies`                 |
| Signed cookies | Verifies and parses signed cookies in `req.signedCookies` |
| Lightweight    | Doesnâ€™t modify response unless needed                     |

**Middleware in Express.js (Node.js)**
 âœ… **Definition**
Middleware is a function that has access to the **request (`req`)**, **response (`res`)**, and the **next middleware function** in the applicationâ€™s request-response cycle.

### ðŸ” **Syntax**

function middleware(req, res, next) {

  next(); // it is a flag that pass control to the next middleware
}

**Usage in Express**

app.use(middlewareFunction);

**Example**

const logger = (req, res, next) => {
  console.log(`${req.method} ${req.url}`);
  next(); // Proceed to next middleware/route
};

app.use(logger);

**Types of Middleware**

| Type                  | Purpose                                   |
| --------------------- | ----------------------------------------- |
| **Application-level** | Attached via `app.use()`                  |
| **Router-level**      | Attached to Express routers               |
| **Built-in**          | Like `express.json()`, `express.static()` |
| **Error-handling**    | With 4 arguments: `(err, req, res, next)` |
| **Third-party**       | Like `cookie-parser`, `morgan`, etc.      |

 **Common Built-in Middleware**


app.use(express.json());           // Parses JSON bodies
app.use(express.urlencoded());     // Parses form data
app.use(express.static("public")); // Serves static files
**Why Middleware?**

 Logging
 Authentication
 Body parsing
 Error handling
 Serving static files
 Modular & readable code


//* User and video model with hooks and JWT
create a User.modal.js, video.model.js in models folder


Using `mongoose-aggregate-paginate-v2` for Pagination in Mongoose

ðŸ“Œ Installation
npm install mongoose-aggregate-paginate-v2

ðŸ“ Setup

1. Import and Apply the Plugin to Your Schema

const mongooseAggregatePaginate = require('mongoose-aggregate-paginate-v2');
// Apply plugin to schema
YourSchema.plugin(mongooseAggregatePaginate);

ðŸ”„ Pagination with Aggregation

Pagination using this plugin involves **two main steps**:

1. Build Aggregation Pipeline

   * Define your aggregation stages (e.g., `$match`, `$lookup`, `$sort`, `$project`, etc.)
   const pipeline = [
     { $match: { status: 'active' } },
     { $sort: { createdAt: -1 } }
     // Add more stages as needed
   ];

2. **Define Pagination Options**

   * Options like page number, limit, and custom labels

   const options = {
     page: 1,
     limit: 10,
     customLabels: {
       totalDocs: 'totalItems',
       docs: 'itemsList',
       limit: 'perPage',
       page: 'currentPage',
       nextPage: 'next',
       prevPage: 'prev',
       totalPages: 'pageCount',
       pagingCounter: 'slNo',
       meta: 'paginator'
     }
   };
   
 ðŸ§ª Example Usage

YourModel.aggregatePaginate(YourModel.aggregate(pipeline), options)
  .then((result) => {
    console.log(result);
  })
  .catch((err) => {
    console.error(err);
  });

ðŸ“š Learn More

You can read more about aggregation in the official MongoDB docs:
ðŸ‘‰ [https://www.mongodb.com/docs/manual/aggregation/](https://www.mongodb.com/docs/manual/aggregation/)

And more on the plugin here:
ðŸ‘‰ [https://www.npmjs.com/package/mongoose-aggregate-paginate-v2](https://www.npmjs.com/package/mongoose-aggregate-paginate-v2)

** bcrypt
node.bcrypt.js
library used to help you hash passwords

npm i bcrypt

**JWT(json web token)


A JSON Web Token (JWT) is a compact, self-contained way to securely transmit information between parties as a JSON object. Itâ€™s digitally signed, so the receiver can verify the dataâ€™s integrity and authenticity. JWTs are commonly signed using either:

* A **shared secret** with HMAC (e.g., HS256), or
* A **public/private key pair** with RSA or ECDSA.

 Why use JWT?

Authorization: After logging in, a JWT is sent with each request to authenticate the user and control access to resources. Itâ€™s widely used in Single Sign-On (SSO) because JWTs are lightweight and work well across domains.

Information Exchange: JWTs can securely transmit data because the signature ensures the data wasnâ€™t tampered with and confirms the senderâ€™s identity.

***JWT Structure

A JWT has **three parts**, each Base64Url encoded and separated by dots (`.`):

```
xxxxx.yyyyy.zzzzz
```

1. **Header:**
   Contains metadata about the token, like the type (`JWT`) and signing algorithm (e.g., `HS256`):

   
   {
     "alg": "HS256",
     "typ": "JWT"
   }
   

2. **Payload:**
   Contains **claims** â€” statements about an entity (usually the user) and additional data. There are three types of claims:

 Registered claims:** Predefined, recommended claims like `iss` (issuer), `exp` (expiration), `sub` (subject), and `aud` (audience).
   
 Public claims:** Custom claims defined by users but registered to avoid collisions.

 Private claims:** Custom claims agreed upon by parties but not registered or public.

   Example:

   {
     "sub": "1234567890",
     "name": "John Doe",
     "admin": true
   }
   

   *Important: The payload is readable by anyone since itâ€™s only Base64Url encoded, not encrypted. Donâ€™t put secrets here unless you encrypt the JWT.

3. Signature:
   Created by signing the encoded header and payload with a secret or private key using the specified algorithm, e.g., HMAC SHA256:

   ```
   HMACSHA256(
     base64UrlEncode(header) + "." +
     base64UrlEncode(payload),
     secret)
   ```

   This signature verifies the tokenâ€™s integrity and authenticity.

//lets install bcrypt  and jsonwebtoken
npm install bcrypt jsonwebtoken
//lets create a user model
import jwt from "jsonwebtoken"
import bcrypt from "bcrypt"

we can not directly bcrypt, so we use pre hook

jwt is a beared token means which person have jwt,he/she get the data only

//* lets create tokens in .env file
then generate them in user.models.js with help of jwt.sign({payload},accessToken,object)

//*file uploading
woking with cloudinary- it is a cloud service
// lets install cloudinary
npm install cloudinary
-> create a cloudinary.js in utils folder

// Cloudinary Utility Notes
// -----------------------------------------------------
// Cloudinary is a cloud-based service for managing images and videos, providing storage, transformation, and delivery.
// We use it to offload media storage from our server, enabling scalable and efficient media handling.
//
// The cloudinary.js utility sets up and exports a helper function to upload files to Cloudinary.
// Configuration uses environment variables for security and flexibility:
//   - CLOUDINARY_CLOUD_NAME
//   - CLOUDINARY_API_KEY
//   - CLOUDINARY_API_SECRET
//
// Usage:
//   import { uploadOnCloudinary } from '../utils/cloudinary';
//   const result = await uploadOnCloudinary(localFilePath);
//
// The function uploadOnCloudinary(localFilePath):
//   - Uploads a local file to Cloudinary (images, videos, etc.)
//   - Returns the Cloudinary response object if successful, or null if failed
//   - On error, removes the local temp file to avoid clutter
//
// Example:
//   const response = await uploadOnCloudinary('/path/to/file.jpg');
//   if (response) {
//     console.log('File uploaded:', response.url);
//   }
//
// The utility uses 'resource_type: auto' to allow uploading images, videos, and other file types automatically.
//
// Make sure to install the cloudinary package:
//   npm install cloudinary
//
// And set the required environment variables in your .env file.


we will upload the file using multer
// lets install multer
npm install multer

we make middleware using multer

// Multer Middleware Notes
// =======================
// Multer is a node.js middleware for handling multipart/form-data, primarily used for uploading files.
// It's written on top of busboy for maximum efficiency and adds file objects to the request.
//
// Key Points:
// - Only processes multipart/form-data forms
// - Adds 'file' or 'files' object to req object
// - Adds 'body' object containing text fields to req object
//
// Installation:
//   npm install multer
//
// Configuration in multer.middleware.js:
// -------------------------------------
// 1. Disk Storage Setup:
//    - destination: Function that determines where to store uploaded files
//    - filename: Function that determines the name of uploaded files
//    - Uses ./public/temp as temporary storage before cloud upload
//
// 2. Multer Instance:
//    - Configured with disk storage
//    - Can add limits, fileFilter, and other options
//
// Usage Examples:
// --------------
// Single file upload:
//   router.post('/upload', upload.single('avatar'), (req, res) => {
//     // req.file contains the uploaded file
//     // req.body contains the text fields
//   });
//
// Multiple files upload:
//   router.post('/upload', upload.array('photos', 5), (req, res) => {
//     // req.files contains array of uploaded files
//     // req.body contains the text fields
//   });
//
// Multiple fields with different file counts:
//   router.post('/upload', upload.fields([
//     { name: 'avatar', maxCount: 1 },
//     { name: 'gallery', maxCount: 8 }
//   ]), (req, res) => {
//     // req.files.avatar[0] - single file
//     // req.files.gallery - array of files
//   });
//
// File Object Properties:
// ----------------------
// When a file is uploaded, req.file contains:
// - fieldname: Form field name
// - originalname: Original file name
// - encoding: File encoding
// - mimetype: MIME type of file
// - size: File size in bytes
// - destination: Where file was saved
// - filename: Name of file on disk
// - path: Full path to file
//
// Important Notes:
// ---------------
// 1. Always clean up temporary files after uploading to cloud storage
// 2. The ./public/temp directory should exist or be created
// 3. Consider adding file type validation in fileFilter
// 4. Set appropriate file size limits for your use case
// 5. This middleware should be used before your route handler
// 6. Works together with Cloudinary for cloud storage
//
// Typical Workflow:
// 1. File uploaded via multer to ./public/temp
// 2. File processed/validated in route handler
// 3. File uploaded to Cloudinary using cloudinary utility
// 4. Temporary file deleted from ./public/temp
// 5. Cloudinary URL stored in database

